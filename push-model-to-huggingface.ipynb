{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b89a00e2",
   "metadata": {},
   "source": [
    "# Loading The Model & Tokenizer to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5691de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aeb5b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /Users/sevilaymuniregirgin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login using the personal token\n",
    "login(token=\"your-token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d3c7db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your local model directory\n",
    "model_dir = \"bert2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86f16954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef643f2be8d4d33bc0abb223ef4ae93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/SevilayG/my-bert-fake-news/commit/62f2039140d3b02937ef7e8d38596d94b13d4a2b', commit_message='Upload tokenizer', commit_description='', oid='62f2039140d3b02937ef7e8d38596d94b13d4a2b', pr_url=None, repo_url=RepoUrl('https://huggingface.co/SevilayG/my-bert-fake-news', endpoint='https://huggingface.co', repo_type='model', repo_id='SevilayG/my-bert-fake-news'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "model.push_to_hub(\"SevilayG/my-bert-fake-news\")\n",
    "tokenizer.push_to_hub(\"SevilayG/my-bert-fake-news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5fca282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sevilaymuniregirgin/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6e5dcabb4b4b2daf08b9f3e63e7257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe9ea1851384e0b9a6efd88ec93e132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6998c0eb08496ea4f03c3de53ecb7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c191b3103f044863b528487f59cf5043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d396c6018f34827907b4378a54e4a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7862b3edc3d428b9d656b8b5ad4e71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"SevilayG/my-bert-fake-news\"\n",
    "my_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "my_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0423ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.4631,  3.4907]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# Test with some text\n",
    "text = \"This is a great day!\"\n",
    "inputs = my_tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = my_model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d71070",
   "metadata": {},
   "source": [
    "## Check the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58a8c3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 15:07:01.227198: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2984ec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text classification pipeline\n",
    "clf = pipeline(\"text-classification\", model=my_model, tokenizer=my_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18be5f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The stock market crashed due to unexpected inflation data.\n",
      "Prediction: {'label': 'LABEL_1', 'score': 0.9559256434440613}\n",
      "\n",
      "Text: Scientists discovered a new species of bird in the Amazon rainforest.\n",
      "Prediction: {'label': 'LABEL_1', 'score': 0.9434421062469482}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example texts for prediction\n",
    "texts = [\n",
    "    \"The stock market crashed due to unexpected inflation data.\",\n",
    "    \"Scientists discovered a new species of bird in the Amazon rainforest.\"]\n",
    "\n",
    "# Get predictions\n",
    "predictions = clf(texts)\n",
    "\n",
    "# Output the predictions\n",
    "for text, prediction in zip(texts, predictions):\n",
    "    print(f\"Text: {text}\\nPrediction: {prediction}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
